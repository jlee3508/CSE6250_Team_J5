{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlee3508/CSE6250_Team_J5/blob/main/CSE6250_Team_J5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyhealth\n",
        "!pip install torchdiffeq\n",
        "!pip install pdb"
      ],
      "metadata": {
        "id": "iNbo58dEdfke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "The background context of the original paper is that predicting and identifying patients at risk of readmission to the intensive care unit is emphasized in contemporary healthcare studies due to its potential impact on early intervention, prevention of readmission, and optimization of healthcare resource allocation. Various approaches, including deep learning architectures, are being explored to enhance the accuracy of predicting readmission within 30 days of discharge from the ICU and identifying at-risk patients using electronic medical records.\n",
        "\n",
        "The original paper proposed and compared different neural network architectures for processing longitudinal EMR data that is recorded at irregular intervals. The architectures proposed involved appending the appending time-related information to the numerical vectors used to represent time stamped codes, to modifying the internal workings of recurrent cells using exponential time-decay functions or ordinary differential equations. The original paper also proposed utilizing medical concept embeddings as well as using neural ODEs to describe how the embedding of a medical code changes over time. The evaluation of these algorithms considered multiple criteria such as average precision, AUROC, F1 score, sensitivity, and specificity. The insights derived from this research aim to assist healthcare providers in understanding ICU patients at increased risk of readmission and improving the efficiency of the entire ICU readmission process.\n",
        "\n",
        "The original paper had fairly consistent results over all of the architectures. They had an average F1 score of 0.37 and an average AUROC of 0.72. The paper demonstrates that the variance among these different architectures is minimal.\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "-------------------------------------------------------------------------------\n",
        "(1) Evaluate the feasibility of using neural ODEs to model how the predictive relevance of recorded medical codes changes over time\n",
        "I will implement and run ODE architecture models to determine the power of ODEs with time data.\n",
        "(2) Perform a comprehensive comparison of deep learning models that have been proposed for processing time-series sampled at irregular intervals, including MCEs, neural ODEs, attention mechanisms, and recurrent layers\n",
        "I will implement various types of architectures and compare them against each other using metrics such as: precision, AUROC, and F1 score\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
        "'''\n",
        "# mount this notebook to your google drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# define dirs to workspace and data\n",
        "img_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-your-image>'\n",
        "\n",
        "#import cv2\n",
        "#img = cv2.imread(img_dir)\n",
        "#cv2.imshow(\"Title\", img)\n"
      ],
      "metadata": {
        "id": "rRksCB1vbYwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import pi\n",
        "from torchdiffeq import odeint, odeint_adjoint\n",
        "from pdb import set_trace as bp\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "import torch.utils.data as utils\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, average_precision_score, roc_auc_score, f1_score\n",
        "from time import time"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is originally from MIMIC 3. Link is as follows: https://physionet.org/content/mimiciii/1.4/\n",
        "The database includes information such as demographics, vital sign measurements made at the bedside, laboratory test results, procedures, medications, caregiver notes, imaging reports, and mortality. There are 45298 patients, 25004496 diagnoses and procedures, and 17756816 charts and prescriptions in the dataset. Out of all patients, 5495 were readmitted and 39803 were not readmitted.\n",
        "The original paper had a 90/10 train/test split. For variables, a variable was synthesized to keep track of the number of ICU stays in a year per patient.\n",
        "\n",
        "This paper deals with multiple types of models and architectures.The following is a list of the architectures tested in the model and a brief description of each.\n",
        "The following neural network architectures were compared for predicting readmission to the ICU:\n",
        "\n",
        "ODE + RNN + Attention: dynamics in time of embeddings are modelled using neural ODEs, embeddings are passed to RNN layers, dot-product attention is applied to RNN outputs.\n",
        "\n",
        "ODE + RNN: dynamics in time of embeddings are modelled using neural ODEs, embeddings are passed to RNN layers, the final memory states are used for further processing.\n",
        "\n",
        "RNN (ODE time decay) + Attention: embeddings are passed to RNN layers with dynamics in time of the internal memory states modelled using neural ODEs, dot-product attention is applied to RNN outputs.\n",
        "\n",
        "RNN (ODE time decay): embeddings are passed to RNN layers with dynamics in time of the internal memory states modelled using neural ODEs, the final memory states are used for further processing.\n",
        "\n",
        "RNN (exp time decay) + Attention: embeddings are passed to RNN layers with internal memory states decaying exponentially over time, dot-product attention is applied to RNN outputs.\n",
        "\n",
        "RNN (exp time decay): embeddings are passed to RNN layers with internal memory states decaying exponentially over time, the final memory states are used for further processing.\n",
        "\n",
        "RNN (concatenated Δtime) + Attention: embeddings are concatenated with time differences between observations and passed to RNN layers, dot-product attention is applied to RNN outputs.\n",
        "\n",
        "RNN (concatenated Δtime): embeddings are concatenated with time differences between observations and passed to RNN layers, the final memory states are used for further processing.\n",
        "\n",
        "ODE + Attention: dynamics in time of embeddings are modelled using neural ODEs, dot-product attention is applied to the embeddings.\n",
        "\n",
        "Attention (concatenated time): embeddings are concatenated with elapsed times, dot-product attention is applied to the embeddings.\n",
        "\n",
        "MCE + RNN + Attention: MCE is used to compute the embeddings, embeddings are passed to RNN layers, dot-product attention is applied to RNN outputs.\n",
        "\n",
        "MCE + RNN: MCE is used to compute the embeddings, embeddings are passed to RNN layers, the final memory states are used for further processing.\n",
        "\n",
        "MCE + Attention: MCE is used to compute the embeddings, dot-product attention is applied to the embeddings.\n",
        "\n",
        "\n",
        "The loss function is binary cross entropy and the optimizer is Adam. The model is not pretrained.\n"
      ],
      "metadata": {
        "id": "XzVUQS0CHry0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_dir = '/content/gdrive/My Drive/cse6250_project_team_j5/data/'\n",
        "logdir = '/content/gdrive/My Drive/cse6250_project_team_j5/logFolder/'\n",
        "modeldir = '/content/gdrive/My Drive/cse6250_project_team_j5/models/'\n",
        "\n",
        "def get_data(data, type):\n",
        "  # Data\n",
        "  static       = data['static'].astype('float32')\n",
        "  label        = data['label'].astype('float32')\n",
        "  dp           = data['dp'].astype('int64') # diagnoses/procedures\n",
        "  cp           = data['cp'].astype('int64') # charts/prescriptions\n",
        "  dp_times     = data['dp_times'].astype('float32')\n",
        "  cp_times     = data['cp_times'].astype('float32')\n",
        "  train_ids    = data['train_ids']\n",
        "  validate_ids = data['validate_ids']\n",
        "  test_ids     = data['test_ids']\n",
        "\n",
        "  if (type == 'TRAIN'):\n",
        "    ids = train_ids\n",
        "  elif (type == 'VALIDATE'):\n",
        "    ids = validate_ids\n",
        "  elif (type == 'TEST'):\n",
        "    ids = test_ids\n",
        "  elif (type == 'ALL'):\n",
        "    ids = np.full_like(label, True, dtype=bool)\n",
        "\n",
        "  static   = static[ids, :]\n",
        "  label    = label[ids]\n",
        "  dp       = dp[ids, :]\n",
        "  cp       = cp[ids, :]\n",
        "  dp_times = dp_times[ids, :]\n",
        "  cp_times = cp_times[ids, :]\n",
        "\n",
        "  return static, dp, cp, dp_times, cp_times, label\n",
        "\n",
        "\n",
        "def get_dictionaries(data):\n",
        "  return data['static_vars'], data['dict_dp'][()], data['dict_cp'][()]\n",
        "\n",
        "\n",
        "def num_statics(data):\n",
        "  return data['static_vars'].shape[0]\n",
        "\n",
        "\n",
        "def vocab_sizes(data):\n",
        "  return data['dp'].max()+1, data['cp'].max()+1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "ilpx6TEIcUjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_count = 100 # words whose occurred less than min_cnt are encoded as OTHER\n",
        "\n",
        "# training\n",
        "batch_size = 64\n",
        "num_epochs = 1\n",
        "dropout_rate = 0.5\n",
        "patience = 10 # early stopping\n",
        "\n",
        "# which data to load\n",
        "# on_the_cloud = False\n",
        "#all_train = False\n",
        "# all_train = True\n",
        "\n",
        "# network variants\n",
        "#net_variant = 'birnn_concat_time_delta'\n",
        "# net_variant = 'birnn_concat_time_delta_attention'\n",
        "#net_variant = 'birnn_time_decay'\n",
        "#net_variant = 'birnn_time_decay_attention'\n",
        "#net_variant = 'ode_birnn'\n",
        "#net_variant = 'ode_birnn_attention'\n",
        "#net_variant = 'ode_attention'\n",
        "#net_variant = 'attention_concat_time'\n",
        "#net_variant = 'birnn_ode_decay'\n",
        "#net_variant = 'birnn_ode_decay_attention'\n",
        "net_variant = 'mce_attention'\n",
        "#net_variant = 'mce_birnn'\n",
        "#net_variant = 'mce_birnn_attention'\n",
        "\n",
        "# bootstrapping\n",
        "np_seed = 1234\n",
        "bootstrap_samples = 2\n",
        "\n",
        "# bayesian network\n",
        "pi = 0.5\n",
        "sigma1 = math.exp(-0)\n",
        "sigma2 = math.exp(-6)\n",
        "samples = 1\n",
        "test_samples = 10"
      ],
      "metadata": {
        "id": "mhp-32ruO7y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trainloader(data, type, shuffle=True, idx=None):\n",
        "  # Data\n",
        "  static, dp, cp, dp_times, cp_times, label = get_data(data, type)\n",
        "\n",
        "  # Bootstrap\n",
        "  if idx is not None:\n",
        "    static, dp, cp, dp_times, cp_times, label = static[idx], dp[idx], cp[idx], dp_times[idx], cp_times[idx], label[idx]\n",
        "\n",
        "  # Compute total batch count\n",
        "  num_batches = len(label) // batch_size\n",
        "\n",
        "  # Create dataset\n",
        "  dataset = utils.TensorDataset(torch.from_numpy(static),\n",
        "                                torch.from_numpy(dp),\n",
        "                                torch.from_numpy(cp),\n",
        "                                torch.from_numpy(dp_times),\n",
        "                                torch.from_numpy(cp_times),\n",
        "                                torch.from_numpy(label))\n",
        "\n",
        "  # Create batch queues\n",
        "  trainloader = utils.DataLoader(dataset,\n",
        "                                 batch_size = batch_size,\n",
        "                                 shuffle = shuffle,\n",
        "                                 sampler = None,\n",
        "                                 num_workers = 2,\n",
        "                                 drop_last = True)\n",
        "\n",
        "  # Weight of positive samples for training\n",
        "  pos_weight = torch.tensor((len(label) - np.sum(label))/np.sum(label))\n",
        "\n",
        "  return trainloader, num_batches, pos_weight"
      ],
      "metadata": {
        "id": "2HkbfL7fOIZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Load data...')\n",
        "data = np.load(data_dir + 'data_arrays.npz')\n",
        "trainloader, num_batches, pos_weight = get_trainloader(data, 'TRAIN')\n",
        "print(\"Data Loaded\")"
      ],
      "metadata": {
        "id": "IMahfNWmOJEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Models\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cell Architecture"
      ],
      "metadata": {
        "id": "YMciqotSCq90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ODEFunc(nn.Module):\n",
        "    \"\"\"MLP modeling the derivative of ODE system.\n",
        "    Parameters\n",
        "    ----------\n",
        "    device : torch.device\n",
        "    data_dim : int\n",
        "        Dimension of data.\n",
        "    hidden_dim : int\n",
        "        Dimension of hidden layers.\n",
        "    augment_dim: int\n",
        "        Dimension of augmentation. If 0 does not augment ODE, otherwise augments\n",
        "        it with augment_dim dimensions.\n",
        "    time_dependent : bool\n",
        "        If True adds time as input, making ODE time dependent.\n",
        "    non_linearity : string\n",
        "        One of 'relu' and 'softplus'\n",
        "    \"\"\"\n",
        "    def __init__(self, device, data_dim, hidden_dim, augment_dim=0,\n",
        "                 time_dependent=False, non_linearity='relu'):\n",
        "        super(ODEFunc, self).__init__()\n",
        "        self.device = device\n",
        "        self.augment_dim = augment_dim\n",
        "        self.data_dim = data_dim\n",
        "        self.input_dim = data_dim + augment_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.nfe = 0  # Number of function evaluations\n",
        "        self.time_dependent = time_dependent\n",
        "\n",
        "        if time_dependent:\n",
        "            self.fc1 = nn.Linear(self.input_dim + 1, hidden_dim)\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(self.input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, self.input_dim)\n",
        "\n",
        "        if non_linearity == 'relu':\n",
        "            self.non_linearity = nn.ReLU(inplace=True)\n",
        "        elif non_linearity == 'softplus':\n",
        "            self.non_linearity = nn.Softplus()\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        t : torch.Tensor\n",
        "            Current time. Shape (1,).\n",
        "        x : torch.Tensor\n",
        "            Shape (batch_size, input_dim)\n",
        "        \"\"\"\n",
        "        # Forward pass of model corresponds to one function evaluation, so\n",
        "        # increment counter\n",
        "        self.nfe += 1\n",
        "        if self.time_dependent:\n",
        "            # Shape (batch_size, 1)\n",
        "            t_vec = torch.ones(x.shape[0], 1).to(self.device) * t\n",
        "            # Shape (batch_size, data_dim + 1)\n",
        "            t_and_x = torch.cat([t_vec, x], 1)\n",
        "            # Shape (batch_size, hidden_dim)\n",
        "            out = self.fc1(t_and_x)\n",
        "        else:\n",
        "            out = self.fc1(x)\n",
        "        out = self.non_linearity(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.non_linearity(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ODEBlock(nn.Module):\n",
        "    \"\"\"Solves ODE defined by odefunc.\n",
        "    Parameters\n",
        "    ----------\n",
        "    device : torch.device\n",
        "    odefunc : ODEFunc instance or anode.conv_models.ConvODEFunc instance\n",
        "        Function defining dynamics of system.\n",
        "    is_conv : bool\n",
        "        If True, treats odefunc as a convolutional model.\n",
        "    tol : float\n",
        "        Error tolerance.\n",
        "    adjoint : bool\n",
        "        If True calculates gradient with adjoint method, otherwise\n",
        "        backpropagates directly through operations of ODE solver.\n",
        "    \"\"\"\n",
        "    def __init__(self, device, odefunc, is_conv=False, tol=1e-3, adjoint=False):\n",
        "        super(ODEBlock, self).__init__()\n",
        "        self.adjoint = adjoint\n",
        "        self.device = device\n",
        "        self.is_conv = is_conv\n",
        "        self.odefunc = odefunc\n",
        "        self.tol = tol\n",
        "\n",
        "    def forward(self, x, eval_times=None):\n",
        "        \"\"\"Solves ODE starting from x.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Shape (batch_size, self.odefunc.data_dim)\n",
        "        eval_times : None or torch.Tensor\n",
        "            If None, returns solution of ODE at final time t=1. If torch.Tensor\n",
        "            then returns full ODE trajectory evaluated at points in eval_times.\n",
        "        \"\"\"\n",
        "        # Forward pass corresponds to solving ODE, so reset number of function\n",
        "        self.odefunc.nfe = 0\n",
        "\n",
        "        if eval_times is None:\n",
        "            integration_time = torch.tensor([0, 1]).float().type_as(x)\n",
        "        else:\n",
        "            integration_time = eval_times.type_as(x)\n",
        "\n",
        "\n",
        "        if self.odefunc.augment_dim > 0:\n",
        "            if self.is_conv:\n",
        "                batch_size, channels, height, width = x.shape\n",
        "                aug = torch.zeros(batch_size, self.odefunc.augment_dim,\n",
        "                                  height, width).to(self.device)\n",
        "                # Shape (batch_size, channels + augment_dim, height, width)\n",
        "                x_aug = torch.cat([x, aug], 1)\n",
        "            else:\n",
        "                aug = torch.zeros(x.shape[0], self.odefunc.augment_dim).to(self.device)\n",
        "                # Shape (batch_size, data_dim + augment_dim)\n",
        "                x_aug = torch.cat([x, aug], 1)\n",
        "        else:\n",
        "            x_aug = x\n",
        "\n",
        "        if self.adjoint:\n",
        "            out = odeint_adjoint(self.odefunc, x_aug, integration_time,\n",
        "                                 rtol=self.tol, atol=self.tol, method='euler')\n",
        "        else:\n",
        "            out = odeint(self.odefunc, x_aug, integration_time,\n",
        "                         rtol=self.tol, atol=self.tol, method='euler')\n",
        "\n",
        "        if eval_times is None:\n",
        "            return out[1]\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class ODENet(nn.Module):\n",
        "    \"\"\"An ODEBlock followed by a Linear layer.\n",
        "    Parameters\n",
        "    ----------\n",
        "    device : torch.device\n",
        "    data_dim : int\n",
        "        Dimension of data.\n",
        "    hidden_dim : int\n",
        "        Dimension of hidden layers.\n",
        "    output_dim : int\n",
        "        Dimension of output after hidden layer. Should be 1 for regression or\n",
        "        num_classes for classification.\n",
        "    augment_dim: int\n",
        "        Dimension of augmentation. If 0 does not augment ODE, otherwise augments\n",
        "        it with augment_dim dimensions.\n",
        "    time_dependent : bool\n",
        "        If True adds time as input, making ODE time dependent.\n",
        "    non_linearity : string\n",
        "        One of 'relu' and 'softplus'\n",
        "    tol : float\n",
        "        Error tolerance.\n",
        "    adjoint : bool\n",
        "        If True calculates gradient with adjoint method, otherwise\n",
        "        backpropagates directly through operations of ODE solver.\n",
        "    \"\"\"\n",
        "    def __init__(self, device, data_dim, hidden_dim, output_dim=1,\n",
        "                 augment_dim=0, time_dependent=False, non_linearity='relu',\n",
        "                 tol=1e-3, adjoint=False):\n",
        "        super(ODENet, self).__init__()\n",
        "        self.device = device\n",
        "        self.data_dim = data_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.augment_dim = augment_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.time_dependent = time_dependent\n",
        "        self.tol = tol\n",
        "\n",
        "        odefunc = ODEFunc(device, data_dim, hidden_dim, augment_dim,\n",
        "                          time_dependent, non_linearity)\n",
        "\n",
        "        self.odeblock = ODEBlock(device, odefunc, tol=tol, adjoint=adjoint)\n",
        "\n",
        "    def forward(self, x, eval_times=None):\n",
        "        features = self.odeblock(x, eval_times)\n",
        "        return features\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Dot-product attention module.\n",
        "\n",
        "  Args:\n",
        "    inputs: A `Tensor` with embeddings in the last dimension.\n",
        "    mask: A `Tensor`. Dimensions are the same as inputs but without the embedding dimension.\n",
        "      Values are 0 for 0-padding in the input and 1 elsewhere.\n",
        "\n",
        "  Returns:\n",
        "    outputs: The input `Tensor` whose embeddings in the last dimension have undergone a weighted average.\n",
        "      The second-last dimension of the `Tensor` is removed.\n",
        "    attention_weights: weights given to each embedding.\n",
        "  \"\"\"\n",
        "  def __init__(self, embedding_dim):\n",
        "    super(Attention, self).__init__()\n",
        "    self.context = nn.Parameter(torch.Tensor(embedding_dim))\n",
        "    self.linear_hidden = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    nn.init.normal_(self.context)\n",
        "\n",
        "  def forward(self, inputs, mask):\n",
        "    hidden = torch.tanh(self.linear_hidden(inputs))\n",
        "    importance = torch.sum(hidden * self.context, dim=-1)\n",
        "    importance = importance.masked_fill(mask == 0, -1e9)\n",
        "    attention_weights = F.softmax(importance, dim=-1)\n",
        "    weighted_projection = inputs * torch.unsqueeze(attention_weights, dim=-1)\n",
        "    outputs = torch.sum(weighted_projection, dim=-2)\n",
        "    return outputs, attention_weights\n",
        "\n",
        "\n",
        "class GRUExponentialDecay(nn.Module):\n",
        "  \"\"\"\n",
        "  GRU RNN module where the hidden state decays exponentially\n",
        "  (see e.g. Che et al. 2018, Recurrent Neural Networks for Multivariate Time Series\n",
        "  with Missing Values).\n",
        "\n",
        "  Args:\n",
        "    inputs: A `Tensor` with embeddings in the last dimension.\n",
        "    times: A `Tensor` with the same shape as inputs containing the recorded times (but no embedding dimension).\n",
        "\n",
        "  Returns:\n",
        "    outs: Hidden states of the RNN.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_size, hidden_size, bias=True):\n",
        "    super(GRUExponentialDecay, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.gru_cell = nn.GRUCell(input_size, hidden_size)\n",
        "    self.decays = nn.Parameter(torch.Tensor(hidden_size)) # exponential decays vector\n",
        "\n",
        "  def forward(self, inputs, times):\n",
        "    if torch.cuda.is_available():\n",
        "      hn = torch.zeros(inputs.size(0), self.hidden_size).cuda() # batch_size x hidden_size\n",
        "      outs = torch.zeros(inputs.size(0), inputs.size(1), self.hidden_size).cuda() # batch_size x seq_len x hidden_size\n",
        "    else:\n",
        "      hn = torch.zeros(inputs.size(0), self.hidden_size) # batch_size x hidden_size\n",
        "      outs = torch.zeros(inputs.size(0), inputs.size(1), self.hidden_size) # batch_size x seq_len x hidden_size\n",
        "\n",
        "    # this is slow\n",
        "    for seq in range(inputs.size(1)):\n",
        "      hn = self.gru_cell(inputs[:,seq,:], hn)\n",
        "      outs[:,seq,:] = hn\n",
        "      hn = hn*torch.exp(-torch.clamp(torch.unsqueeze(times[:,seq], dim=-1)*self.decays, min=0))\n",
        "    return outs\n",
        "\n",
        "\n",
        "class GRUOdeDecay(nn.Module):\n",
        "  \"\"\"\n",
        "  GRU RNN module where the hidden state decays according to an ODE.\n",
        "  (see Rubanova et al. 2019, Latent ODEs for Irregularly-Sampled Time Series)\n",
        "\n",
        "  Args:\n",
        "    inputs: A `Tensor` with embeddings in the last dimension.\n",
        "    times: A `Tensor` with the same shape as inputs containing the recorded times (but no embedding dimension).\n",
        "\n",
        "  Returns:\n",
        "    outs: Hidden states of the RNN.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_size, hidden_size, bias=True):\n",
        "    super(GRUOdeDecay, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.gru_cell = nn.GRUCell(input_size, hidden_size)\n",
        "    self.decays = nn.Parameter(torch.Tensor(hidden_size)) # exponential decays vector\n",
        "\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.ode_net = ODENet(self.device, self.input_size, self.input_size, output_dim=self.input_size, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
        "\n",
        "  def forward(self, inputs, times):\n",
        "    if torch.cuda.is_available():\n",
        "      hn = torch.zeros(inputs.size(0), self.hidden_size).cuda() # batch_size x hidden_size\n",
        "      outs = torch.zeros(inputs.size(0), inputs.size(1), self.hidden_size).cuda() # batch_size x seq_len x hidden_size\n",
        "    else:\n",
        "      hn = torch.zeros(inputs.size(0), self.hidden_size) # batch_size x hidden_size\n",
        "      outs = torch.zeros(inputs.size(0), inputs.size(1), self.hidden_size) # batch_size x seq_len x hidden_size\n",
        "\n",
        "    # this is slow\n",
        "    for seq in range(inputs.size(1)):\n",
        "      hn = self.gru_cell(inputs[:,seq,:], hn)\n",
        "      outs[:,seq,:] = hn\n",
        "\n",
        "      times_unique, inverse_indices = torch.unique(times[:,seq], sorted=True, return_inverse=True)\n",
        "      if times_unique.size(0) > 1:\n",
        "        hn = self.ode_net(hn, times_unique)\n",
        "        hn = hn[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
        "    return outs\n",
        "\n",
        "\n",
        "def abs_time_to_delta(times):\n",
        "  delta = torch.cat((torch.unsqueeze(times[:, 0], dim=-1), times[:, 1:] - times[:, :-1]), dim=1)\n",
        "  delta = torch.clamp(delta, min=0)\n",
        "  return delta\n"
      ],
      "metadata": {
        "id": "6jHnM321P0lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Architectures"
      ],
      "metadata": {
        "id": "C1rxC_PbC3cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if net_variant == 'birnn_time_decay':\n",
        "  # GRU\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
        "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
        "\n",
        "      # Embedding layers\n",
        "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
        "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
        "\n",
        "      # GRU layers\n",
        "      self.gru_dp_fw = GRUExponentialDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
        "      self.gru_cp_fw = GRUExponentialDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
        "      self.gru_dp_bw = GRUExponentialDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
        "      self.gru_cp_bw = GRUExponentialDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
        "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Compute time delta\n",
        "      ## output dim: batch_size x seq_len\n",
        "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
        "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
        "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
        "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))\n",
        "\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp_fw = self.embed_dp(dp)\n",
        "      embedded_cp_fw = self.embed_cp(cp)\n",
        "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
        "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
        "      ## Dropout\n",
        "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
        "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
        "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
        "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
        "\n",
        "      # GRU\n",
        "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
        "      rnn_dp_fw = self.gru_dp_fw(embedded_dp_fw, dp_t_delta_fw)\n",
        "      rnn_cp_fw = self.gru_cp_fw(embedded_cp_fw, cp_t_delta_fw)\n",
        "      rnn_dp_bw = self.gru_dp_bw(embedded_dp_bw, dp_t_delta_bw)\n",
        "      rnn_cp_bw = self.gru_cp_bw(embedded_cp_bw, cp_t_delta_bw)\n",
        "      ## output dim rnn_hidden: batch_size x embedding_dim\n",
        "      rnn_dp_fw = rnn_dp_fw[:,-1,:]\n",
        "      rnn_cp_fw = rnn_cp_fw[:,-1,:]\n",
        "      rnn_dp_bw = rnn_dp_bw[:,-1,:]\n",
        "      rnn_cp_bw = rnn_cp_bw[:,-1,:]\n",
        "      ## concatenate forward and backward: batch_size x 2*embedding_dim\n",
        "      rnn_dp = torch.cat((rnn_dp_fw, rnn_dp_bw), dim=-1)\n",
        "      rnn_cp = torch.cat((rnn_cp_fw, rnn_cp_bw), dim=-1)\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(rnn_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(rnn_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []\n",
        "\n",
        "if net_variant == 'birnn_concat_time_delta':\n",
        "  # GRU\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))\n",
        "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))\n",
        "\n",
        "      # Embedding layers\n",
        "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
        "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
        "\n",
        "      # GRU layers\n",
        "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
        "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
        "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
        "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(2*(self.embed_dp_dim+1), 1)\n",
        "      self.fc_cp  = nn.Linear(2*(self.embed_cp_dim+1), 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Compute time delta\n",
        "      ## output dim: batch_size x seq_len\n",
        "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
        "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
        "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
        "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))\n",
        "\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp_fw = self.embed_dp(dp)\n",
        "      embedded_cp_fw = self.embed_cp(cp)\n",
        "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
        "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
        "\n",
        "      # Concatate with time\n",
        "      ## output dim: batch_size x seq_len x (embedding_dim+1)\n",
        "      concat_dp_fw = torch.cat((embedded_dp_fw, torch.unsqueeze(dp_t_delta_fw, dim=-1)), dim=-1)\n",
        "      concat_cp_fw = torch.cat((embedded_cp_fw, torch.unsqueeze(cp_t_delta_fw, dim=-1)), dim=-1)\n",
        "      concat_dp_bw = torch.cat((embedded_dp_bw, torch.unsqueeze(dp_t_delta_bw, dim=-1)), dim=-1)\n",
        "      concat_cp_bw = torch.cat((embedded_cp_bw, torch.unsqueeze(cp_t_delta_bw, dim=-1)), dim=-1)\n",
        "      ## Dropout\n",
        "      concat_dp_fw = self.dropout(concat_dp_fw)\n",
        "      concat_cp_fw = self.dropout(concat_cp_fw)\n",
        "      concat_dp_bw = self.dropout(concat_dp_bw)\n",
        "      concat_cp_bw = self.dropout(concat_cp_bw)\n",
        "\n",
        "      # GRU\n",
        "      ## output dim rnn:        batch_size x seq_len x (embedding_dim+1)\n",
        "      ## output dim rnn_hidden: batch_size x 1 x (embedding_dim+1)\n",
        "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(concat_dp_fw)\n",
        "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(concat_cp_fw)\n",
        "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(concat_dp_bw)\n",
        "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(concat_cp_bw)\n",
        "      ## output dim rnn_hidden: batch_size x (embedding_dim+1)\n",
        "      rnn_hidden_dp_fw = rnn_hidden_dp_fw.view(-1, self.embed_dp_dim+1)\n",
        "      rnn_hidden_cp_fw = rnn_hidden_cp_fw.view(-1, self.embed_cp_dim+1)\n",
        "      rnn_hidden_dp_bw = rnn_hidden_dp_bw.view(-1, self.embed_dp_dim+1)\n",
        "      rnn_hidden_cp_bw = rnn_hidden_cp_bw.view(-1, self.embed_cp_dim+1)\n",
        "      ## concatenate forward and backward: batch_size x 2*(embedding_dim+1)\n",
        "      rnn_hidden_dp = torch.cat((rnn_hidden_dp_fw, rnn_hidden_dp_bw), dim=-1)\n",
        "      rnn_hidden_cp = torch.cat((rnn_hidden_cp_fw, rnn_hidden_cp_bw), dim=-1)\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(rnn_hidden_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(rnn_hidden_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []\n",
        "\n",
        "\n",
        "elif net_variant == 'birnn_concat_time_delta_attention':\n",
        "  # GRU\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))\n",
        "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))\n",
        "\n",
        "      # Embedding layers\n",
        "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
        "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
        "\n",
        "      # GRU layers\n",
        "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
        "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
        "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
        "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
        "\n",
        "      # Attention layers\n",
        "      self.attention_dp = Attention(embedding_dim=2*(self.embed_dp_dim+1)) #+1 for the concatenated time\n",
        "      self.attention_cp = Attention(embedding_dim=2*(self.embed_cp_dim+1))\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(2*(self.embed_dp_dim+1), 1)\n",
        "      self.fc_cp  = nn.Linear(2*(self.embed_cp_dim+1), 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Compute time delta\n",
        "      ## output dim: batch_size x seq_len\n",
        "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
        "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
        "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
        "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))\n",
        "\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp_fw = self.embed_dp(dp)\n",
        "      embedded_cp_fw = self.embed_cp(cp)\n",
        "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
        "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
        "\n",
        "      # Concatate with time\n",
        "      ## output dim: batch_size x seq_len x (embedding_dim+1)\n",
        "      concat_dp_fw = torch.cat((embedded_dp_fw, torch.unsqueeze(dp_t_delta_fw, dim=-1)), dim=-1)\n",
        "      concat_cp_fw = torch.cat((embedded_cp_fw, torch.unsqueeze(cp_t_delta_fw, dim=-1)), dim=-1)\n",
        "      concat_dp_bw = torch.cat((embedded_dp_bw, torch.unsqueeze(dp_t_delta_bw, dim=-1)), dim=-1)\n",
        "      concat_cp_bw = torch.cat((embedded_cp_bw, torch.unsqueeze(cp_t_delta_bw, dim=-1)), dim=-1)\n",
        "      ## Dropout\n",
        "      concat_dp_fw = self.dropout(concat_dp_fw)\n",
        "      concat_cp_fw = self.dropout(concat_cp_fw)\n",
        "      concat_dp_bw = self.dropout(concat_dp_bw)\n",
        "      concat_cp_bw = self.dropout(concat_cp_bw)\n",
        "\n",
        "      # GRU\n",
        "      ## output dim rnn:        batch_size x seq_len x (embedding_dim+1)\n",
        "      ## output dim rnn_hidden: batch_size x 1 x (embedding_dim+1)\n",
        "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(concat_dp_fw)\n",
        "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(concat_cp_fw)\n",
        "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(concat_dp_bw)\n",
        "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(concat_cp_bw)\n",
        "      # concatenate forward and backward\n",
        "      ## output dim: batch_size x seq_len x 2*(embedding_dim+1)\n",
        "      rnn_dp = torch.cat((rnn_dp_fw, torch.flip(rnn_dp_bw, [1])), dim=-1)\n",
        "      rnn_cp = torch.cat((rnn_cp_fw, torch.flip(rnn_cp_bw, [1])), dim=-1)\n",
        "\n",
        "      # Attention\n",
        "      ## output dim: batch_size x 2*(embedding_dim+1)\n",
        "      attended_dp, weights_dp = self.attention_dp(rnn_dp, (dp > 0).float())\n",
        "      attended_cp, weights_cp = self.attention_cp(rnn_cp, (cp > 0).float())\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []\n",
        "\n",
        "\n",
        "elif net_variant == 'birnn_time_decay':\n",
        "  # GRU\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
        "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
        "\n",
        "      # Embedding layers\n",
        "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
        "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
        "\n",
        "      # GRU layers\n",
        "      self.gru_dp_fw = GRUExponentialDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
        "      self.gru_cp_fw = GRUExponentialDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
        "      self.gru_dp_bw = GRUExponentialDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
        "      self.gru_cp_bw = GRUExponentialDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
        "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Compute time delta\n",
        "      ## output dim: batch_size x seq_len\n",
        "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
        "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
        "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
        "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))\n",
        "\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp_fw = self.embed_dp(dp)\n",
        "      embedded_cp_fw = self.embed_cp(cp)\n",
        "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
        "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
        "      ## Dropout\n",
        "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
        "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
        "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
        "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
        "\n",
        "      # GRU\n",
        "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
        "      rnn_dp_fw = self.gru_dp_fw(embedded_dp_fw, dp_t_delta_fw)\n",
        "      rnn_cp_fw = self.gru_cp_fw(embedded_cp_fw, cp_t_delta_fw)\n",
        "      rnn_dp_bw = self.gru_dp_bw(embedded_dp_bw, dp_t_delta_bw)\n",
        "      rnn_cp_bw = self.gru_cp_bw(embedded_cp_bw, cp_t_delta_bw)\n",
        "      ## output dim rnn_hidden: batch_size x embedding_dim\n",
        "      rnn_dp_fw = rnn_dp_fw[:,-1,:]\n",
        "      rnn_cp_fw = rnn_cp_fw[:,-1,:]\n",
        "      rnn_dp_bw = rnn_dp_bw[:,-1,:]\n",
        "      rnn_cp_bw = rnn_cp_bw[:,-1,:]\n",
        "      ## concatenate forward and backward: batch_size x 2*embedding_dim\n",
        "      rnn_dp = torch.cat((rnn_dp_fw, rnn_dp_bw), dim=-1)\n",
        "      rnn_cp = torch.cat((rnn_cp_fw, rnn_cp_bw), dim=-1)\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(rnn_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(rnn_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []\n",
        "\n",
        "\n",
        "elif net_variant == 'birnn_time_decay_attention':\n",
        "  # GRU\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
        "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
        "\n",
        "      # Embedding layers\n",
        "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
        "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
        "\n",
        "      # GRU layers\n",
        "      self.gru_dp_fw = GRUExponentialDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
        "      self.gru_cp_fw = GRUExponentialDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
        "      self.gru_dp_bw = GRUExponentialDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
        "      self.gru_cp_bw = GRUExponentialDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
        "\n",
        "      # Attention layers\n",
        "      self.attention_dp = Attention(embedding_dim=2*self.embed_dp_dim)\n",
        "      self.attention_cp = Attention(embedding_dim=2*self.embed_cp_dim)\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
        "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Compute time delta\n",
        "      ## output dim: batch_size x seq_len\n",
        "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
        "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
        "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
        "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))\n",
        "\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp_fw = self.embed_dp(dp)\n",
        "      embedded_cp_fw = self.embed_cp(cp)\n",
        "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
        "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
        "      ## Dropout\n",
        "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
        "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
        "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
        "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
        "\n",
        "      # GRU\n",
        "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
        "      rnn_dp_fw = self.gru_dp_fw(embedded_dp_fw, dp_t_delta_fw)\n",
        "      rnn_cp_fw = self.gru_cp_fw(embedded_cp_fw, cp_t_delta_fw)\n",
        "      rnn_dp_bw = self.gru_dp_bw(embedded_dp_bw, dp_t_delta_bw)\n",
        "      rnn_cp_bw = self.gru_cp_bw(embedded_cp_bw, cp_t_delta_bw)\n",
        "      # concatenate forward and backward\n",
        "      ## output dim: batch_size x seq_len x 2*embedding_dim\n",
        "      rnn_dp = torch.cat((rnn_dp_fw, torch.flip(rnn_dp_bw, [1])), dim=-1)\n",
        "      rnn_cp = torch.cat((rnn_cp_fw, torch.flip(rnn_cp_bw, [1])), dim=-1)\n",
        "\n",
        "      # Attention\n",
        "      ## output dim: batch_size x 2*embedding_dim\n",
        "      attended_dp, weights_dp = self.attention_dp(rnn_dp, (dp > 0).float())\n",
        "      attended_cp, weights_cp = self.attention_cp(rnn_cp, (cp > 0).float())\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []\n",
        "\n",
        "\n",
        "elif net_variant == 'attention_concat_time':\n",
        "  # Attention Only\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(2*np.ceil(num_dp_codes**0.25))\n",
        "      self.embed_cp_dim = int(2*np.ceil(num_cp_codes**0.25))\n",
        "\n",
        "      # Embedding layers\n",
        "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
        "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
        "\n",
        "      # Attention layers\n",
        "      self.attention_dp = Attention(embedding_dim=self.embed_dp_dim+1) #+1 for the concatenated time\n",
        "      self.attention_cp = Attention(embedding_dim=self.embed_cp_dim+1)\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(self.embed_dp_dim+1, 1)\n",
        "      self.fc_cp  = nn.Linear(self.embed_cp_dim+1, 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp = self.embed_dp(dp)\n",
        "      embedded_cp = self.embed_cp(cp)\n",
        "\n",
        "      # Concatate with time\n",
        "      ## output dim: batch_size x seq_len x (embedding_dim+1)\n",
        "      concat_dp = torch.cat((embedded_dp, torch.unsqueeze(dp_t, dim=-1)), dim=-1)\n",
        "      concat_cp = torch.cat((embedded_cp, torch.unsqueeze(cp_t, dim=-1)), dim=-1)\n",
        "      ## Dropout\n",
        "      concat_dp = self.dropout(concat_dp)\n",
        "      concat_cp = self.dropout(concat_cp)\n",
        "\n",
        "      # Attention\n",
        "      ## output dim: batch_size x (embedding_dim+1)\n",
        "      attended_dp, weights_dp = self.attention_dp(concat_dp, (dp > 0).float())\n",
        "      attended_cp, weights_cp = self.attention_cp(concat_cp, (cp > 0).float())\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []\n",
        "\n",
        "\n",
        "elif net_variant == 'ode_birnn':\n",
        "  # Attention Only\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
        "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
        "\n",
        "      # Embedding layers\n",
        "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
        "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
        "\n",
        "      # ODE layers\n",
        "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "      self.ode_dp = ODENet(self.device, self.embed_dp_dim, self.embed_dp_dim, output_dim=self.embed_dp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
        "      self.ode_cp = ODENet(self.device, self.embed_cp_dim, self.embed_cp_dim, output_dim=self.embed_cp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
        "\n",
        "      # GRU layers\n",
        "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim, num_layers=1, batch_first=True)\n",
        "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim, num_layers=1, batch_first=True)\n",
        "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim, num_layers=1, batch_first=True)\n",
        "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim, num_layers=1, batch_first=True)\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
        "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp = self.embed_dp(dp)\n",
        "      embedded_cp = self.embed_cp(cp)\n",
        "\n",
        "      # ODE\n",
        "      ## Round times\n",
        "      dp_t = torch.round(100*dp_t)/100\n",
        "      cp_t = torch.round(100*cp_t)/100\n",
        "\n",
        "      embedded_dp_long = embedded_dp.view(-1, self.embed_dp_dim)\n",
        "      dp_t_long = dp_t.view(-1)\n",
        "      dp_t_long_unique, inverse_indices = torch.unique(dp_t_long, sorted=True, return_inverse=True)\n",
        "      ode_dp_long = self.ode_dp(embedded_dp_long, dp_t_long_unique)\n",
        "      ode_dp_long = ode_dp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
        "      ode_dp = ode_dp_long.view(dp.size(0), dp.size(1), self.embed_dp_dim)\n",
        "\n",
        "      embedded_cp_long = embedded_cp.view(-1, self.embed_cp_dim)\n",
        "      cp_t_long = cp_t.view(-1)\n",
        "      cp_t_long_unique, inverse_indices = torch.unique(cp_t_long, sorted=True, return_inverse=True)\n",
        "      ode_cp_long = self.ode_cp(embedded_cp_long, cp_t_long_unique)\n",
        "      ode_cp_long = ode_cp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
        "      ode_cp = ode_cp_long.view(cp.size(0), cp.size(1), self.embed_cp_dim)\n",
        "\n",
        "      ## Dropout\n",
        "      ode_dp = self.dropout(ode_dp)\n",
        "      ode_cp = self.dropout(ode_cp)\n",
        "\n",
        "      # Forward and backward sequences\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      ode_dp_fw = ode_dp\n",
        "      ode_cp_fw = ode_cp\n",
        "      ode_dp_bw = torch.flip(ode_dp_fw, [1])\n",
        "      ode_cp_bw = torch.flip(ode_cp_fw, [1])\n",
        "\n",
        "      # GRU\n",
        "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
        "      ## output dim rnn_hidden: batch_size x 1 x embedding_dim\n",
        "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(ode_dp_fw)\n",
        "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(ode_cp_fw)\n",
        "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(ode_dp_bw)\n",
        "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(ode_cp_bw)\n",
        "      ## output dim rnn_hidden: batch_size x embedding_dim\n",
        "      rnn_hidden_dp_fw = rnn_hidden_dp_fw.view(-1, self.embed_dp_dim)\n",
        "      rnn_hidden_cp_fw = rnn_hidden_cp_fw.view(-1, self.embed_cp_dim)\n",
        "      rnn_hidden_dp_bw = rnn_hidden_dp_bw.view(-1, self.embed_dp_dim)\n",
        "      rnn_hidden_cp_bw = rnn_hidden_cp_bw.view(-1, self.embed_cp_dim)\n",
        "      ## concatenate forward and backward: batch_size x 2*embedding_dim\n",
        "      rnn_hidden_dp = torch.cat((rnn_hidden_dp_fw, rnn_hidden_dp_bw), dim=-1)\n",
        "      rnn_hidden_cp = torch.cat((rnn_hidden_cp_fw, rnn_hidden_cp_bw), dim=-1)\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(rnn_hidden_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(rnn_hidden_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []\n",
        "\n",
        "\n",
        "elif net_variant == 'ode_birnn_attention':\n",
        "  # Attention Only\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
        "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
        "\n",
        "      # Embedding layers\n",
        "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
        "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
        "\n",
        "      # ODE layers\n",
        "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "      self.ode_dp = ODENet(self.device, self.embed_dp_dim, self.embed_dp_dim, output_dim=self.embed_dp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
        "      self.ode_cp = ODENet(self.device, self.embed_cp_dim, self.embed_cp_dim, output_dim=self.embed_cp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
        "\n",
        "      # GRU layers\n",
        "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim, num_layers=1, batch_first=True)\n",
        "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim, num_layers=1, batch_first=True)\n",
        "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim, num_layers=1, batch_first=True)\n",
        "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim, num_layers=1, batch_first=True)\n",
        "\n",
        "      # Attention layers\n",
        "      self.attention_dp = Attention(embedding_dim=2*self.embed_dp_dim)\n",
        "      self.attention_cp = Attention(embedding_dim=2*self.embed_cp_dim)\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
        "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp = self.embed_dp(dp)\n",
        "      embedded_cp = self.embed_cp(cp)\n",
        "\n",
        "      # ODE\n",
        "      ## Round times\n",
        "      dp_t = torch.round(100*dp_t)/100\n",
        "      cp_t = torch.round(100*cp_t)/100\n",
        "\n",
        "      embedded_dp_long = embedded_dp.view(-1, self.embed_dp_dim)\n",
        "      dp_t_long = dp_t.view(-1)\n",
        "      dp_t_long_unique, inverse_indices = torch.unique(dp_t_long, sorted=True, return_inverse=True)\n",
        "      ode_dp_long = self.ode_dp(embedded_dp_long, dp_t_long_unique)\n",
        "      ode_dp_long = ode_dp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
        "      ode_dp = ode_dp_long.view(dp.size(0), dp.size(1), self.embed_dp_dim)\n",
        "\n",
        "      embedded_cp_long = embedded_cp.view(-1, self.embed_cp_dim)\n",
        "      cp_t_long = cp_t.view(-1)\n",
        "      cp_t_long_unique, inverse_indices = torch.unique(cp_t_long, sorted=True, return_inverse=True)\n",
        "      ode_cp_long = self.ode_cp(embedded_cp_long, cp_t_long_unique)\n",
        "      ode_cp_long = ode_cp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
        "      ode_cp = ode_cp_long.view(cp.size(0), cp.size(1), self.embed_cp_dim)\n",
        "\n",
        "      ## Dropout\n",
        "      ode_dp = self.dropout(ode_dp)\n",
        "      ode_cp = self.dropout(ode_cp)\n",
        "\n",
        "      # Forward and backward sequences\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      ode_dp_fw = ode_dp\n",
        "      ode_cp_fw = ode_cp\n",
        "      ode_dp_bw = torch.flip(ode_dp_fw, [1])\n",
        "      ode_cp_bw = torch.flip(ode_cp_fw, [1])\n",
        "\n",
        "      # GRU\n",
        "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
        "      ## output dim rnn_hidden: batch_size x 1 x embedding_dim\n",
        "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(ode_dp_fw)\n",
        "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(ode_cp_fw)\n",
        "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(ode_dp_bw)\n",
        "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(ode_cp_bw)\n",
        "      # concatenate forward and backward\n",
        "      ## output dim: batch_size x seq_len x 2*embedding_dim\n",
        "      rnn_dp = torch.cat((rnn_dp_fw, torch.flip(rnn_dp_bw, [1])), dim=-1)\n",
        "      rnn_cp = torch.cat((rnn_cp_fw, torch.flip(rnn_cp_bw, [1])), dim=-1)\n",
        "\n",
        "      # Attention\n",
        "      ## output dim: batch_size x 2*embedding_dim\n",
        "      attended_dp, weights_dp = self.attention_dp(rnn_dp, (dp > 0).float())\n",
        "      attended_cp, weights_cp = self.attention_cp(rnn_cp, (cp > 0).float())\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []\n",
        "\n",
        "\n",
        "elif net_variant == 'ode_attention':\n",
        "  # Attention Only\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(2*np.ceil(num_dp_codes**0.25))+1\n",
        "      self.embed_cp_dim = int(2*np.ceil(num_cp_codes**0.25))+1\n",
        "\n",
        "      # Embedding layers\n",
        "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
        "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
        "\n",
        "      # ODE layers\n",
        "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "      self.ode_dp = ODENet(self.device, self.embed_dp_dim, self.embed_dp_dim, output_dim=self.embed_dp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
        "      self.ode_cp = ODENet(self.device, self.embed_cp_dim, self.embed_cp_dim, output_dim=self.embed_cp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
        "\n",
        "      # Attention layers\n",
        "      self.attention_dp = Attention(embedding_dim=self.embed_dp_dim)\n",
        "      self.attention_cp = Attention(embedding_dim=self.embed_cp_dim)\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(self.embed_dp_dim, 1)\n",
        "      self.fc_cp  = nn.Linear(self.embed_cp_dim, 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp = self.embed_dp(dp)\n",
        "      embedded_cp = self.embed_cp(cp)\n",
        "\n",
        "      # ODE\n",
        "      ## Round times\n",
        "      dp_t = torch.round(100*dp_t)/100\n",
        "      cp_t = torch.round(100*cp_t)/100\n",
        "\n",
        "      embedded_dp_long = embedded_dp.view(-1, self.embed_dp_dim)\n",
        "      dp_t_long = dp_t.view(-1)\n",
        "      dp_t_long_unique, inverse_indices = torch.unique(dp_t_long, sorted=True, return_inverse=True)\n",
        "      ode_dp_long = self.ode_dp(embedded_dp_long, dp_t_long_unique)\n",
        "      ode_dp_long = ode_dp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
        "      ode_dp = ode_dp_long.view(dp.size(0), dp.size(1), self.embed_dp_dim)\n",
        "\n",
        "      embedded_cp_long = embedded_cp.view(-1, self.embed_cp_dim)\n",
        "      cp_t_long = cp_t.view(-1)\n",
        "      cp_t_long_unique, inverse_indices = torch.unique(cp_t_long, sorted=True, return_inverse=True)\n",
        "      ode_cp_long = self.ode_cp(embedded_cp_long, cp_t_long_unique)\n",
        "      ode_cp_long = ode_cp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
        "      ode_cp = ode_cp_long.view(cp.size(0), cp.size(1), self.embed_cp_dim)\n",
        "\n",
        "      ## Dropout\n",
        "      ode_dp = self.dropout(ode_dp)\n",
        "      ode_cp = self.dropout(ode_cp)\n",
        "\n",
        "      # Attention\n",
        "      ## output dim: batch_size x (embedding_dim+1)\n",
        "      attended_dp, weights_dp = self.attention_dp(ode_dp, (dp > 0).float())\n",
        "      attended_cp, weights_cp = self.attention_cp(ode_cp, (cp > 0).float())\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []\n",
        "\n",
        "\n",
        "elif net_variant == 'birnn_ode_decay':\n",
        "  # GRU\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
        "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
        "\n",
        "      # Embedding layers\n",
        "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
        "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
        "\n",
        "      # GRU layers\n",
        "      self.gru_dp_fw = GRUOdeDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
        "      self.gru_cp_fw = GRUOdeDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
        "      self.gru_dp_bw = GRUOdeDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
        "      self.gru_cp_bw = GRUOdeDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
        "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Compute time delta\n",
        "      ## output dim: batch_size x seq_len\n",
        "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
        "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
        "      ## Round\n",
        "      dp_t_delta_fw = torch.round(100*dp_t_delta_fw)/100\n",
        "      cp_t_delta_fw = torch.round(100*cp_t_delta_fw)/100\n",
        "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
        "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))\n",
        "\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp_fw = self.embed_dp(dp)\n",
        "      embedded_cp_fw = self.embed_cp(cp)\n",
        "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
        "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
        "      ## Dropout\n",
        "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
        "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
        "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
        "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
        "\n",
        "      # GRU\n",
        "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
        "      rnn_dp_fw = self.gru_dp_fw(embedded_dp_fw, dp_t_delta_fw)\n",
        "      rnn_cp_fw = self.gru_cp_fw(embedded_cp_fw, cp_t_delta_fw)\n",
        "      rnn_dp_bw = self.gru_dp_bw(embedded_dp_bw, dp_t_delta_bw)\n",
        "      rnn_cp_bw = self.gru_cp_bw(embedded_cp_bw, cp_t_delta_bw)\n",
        "      ## output dim rnn_hidden: batch_size x embedding_dim\n",
        "      rnn_dp_fw = rnn_dp_fw[:,-1,:]\n",
        "      rnn_cp_fw = rnn_cp_fw[:,-1,:]\n",
        "      rnn_dp_bw = rnn_dp_bw[:,-1,:]\n",
        "      rnn_cp_bw = rnn_cp_bw[:,-1,:]\n",
        "      ## concatenate forward and backward: batch_size x 2*embedding_dim\n",
        "      rnn_dp = torch.cat((rnn_dp_fw, rnn_dp_bw), dim=-1)\n",
        "      rnn_cp = torch.cat((rnn_cp_fw, rnn_cp_bw), dim=-1)\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(rnn_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(rnn_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []\n",
        "\n",
        "\n",
        "elif net_variant == 'birnn_ode_decay_attention':\n",
        "  # GRU\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
        "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
        "\n",
        "      # Embedding layers\n",
        "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
        "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
        "\n",
        "      # GRU layers\n",
        "      self.gru_dp_fw = GRUOdeDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
        "      self.gru_cp_fw = GRUOdeDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
        "      self.gru_dp_bw = GRUOdeDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
        "      self.gru_cp_bw = GRUOdeDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
        "\n",
        "      # Attention layers\n",
        "      self.attention_dp = Attention(embedding_dim=2*self.embed_dp_dim)\n",
        "      self.attention_cp = Attention(embedding_dim=2*self.embed_cp_dim)\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
        "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Compute time delta\n",
        "      ## output dim: batch_size x seq_len\n",
        "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
        "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
        "      ## Round\n",
        "      dp_t_delta_fw = torch.round(100*dp_t_delta_fw)/100\n",
        "      cp_t_delta_fw = torch.round(100*cp_t_delta_fw)/100\n",
        "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
        "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))\n",
        "\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp_fw = self.embed_dp(dp)\n",
        "      embedded_cp_fw = self.embed_cp(cp)\n",
        "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
        "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
        "      ## Dropout\n",
        "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
        "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
        "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
        "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
        "\n",
        "      # GRU\n",
        "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
        "      rnn_dp_fw = self.gru_dp_fw(embedded_dp_fw, dp_t_delta_fw)\n",
        "      rnn_cp_fw = self.gru_cp_fw(embedded_cp_fw, cp_t_delta_fw)\n",
        "      rnn_dp_bw = self.gru_dp_bw(embedded_dp_bw, dp_t_delta_bw)\n",
        "      rnn_cp_bw = self.gru_cp_bw(embedded_cp_bw, cp_t_delta_bw)\n",
        "      # concatenate forward and backward\n",
        "      ## output dim: batch_size x seq_len x 2*embedding_dim\n",
        "      rnn_dp = torch.cat((rnn_dp_fw, torch.flip(rnn_dp_bw, [1])), dim=-1)\n",
        "      rnn_cp = torch.cat((rnn_cp_fw, torch.flip(rnn_cp_bw, [1])), dim=-1)\n",
        "\n",
        "      # Attention\n",
        "      ## output dim: batch_size x 2*embedding_dim\n",
        "      attended_dp, weights_dp = self.attention_dp(rnn_dp, (dp > 0).float())\n",
        "      attended_cp, weights_cp = self.attention_cp(rnn_cp, (cp > 0).float())\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []\n",
        "\n",
        "\n",
        "elif net_variant == 'mce_attention':\n",
        "  # Attention Only\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(2*np.ceil(num_dp_codes**0.25))\n",
        "      self.embed_cp_dim = int(2*np.ceil(num_cp_codes**0.25))\n",
        "\n",
        "      # Precomputed embedding weights\n",
        "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "      self.emb_weight_dp = torch.Tensor(np.load(data_dir + 'emb_weight_dp_13.npy')).to(self.device)\n",
        "      self.emb_weight_cp = torch.Tensor(np.load(data_dir + 'emb_weight_cp_11.npy')).to(self.device)\n",
        "\n",
        "      # Attention layers\n",
        "      self.attention_dp = Attention(embedding_dim=self.embed_dp_dim+1) #+1 for the concatenated time\n",
        "      self.attention_cp = Attention(embedding_dim=self.embed_cp_dim+1)\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(self.embed_dp_dim+1, 1)\n",
        "      self.fc_cp  = nn.Linear(self.embed_cp_dim+1, 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp = F.embedding(dp, self.emb_weight_dp, padding_idx=0)\n",
        "      embedded_cp = F.embedding(cp, self.emb_weight_cp, padding_idx=0)\n",
        "      ## Dropout\n",
        "      embedded_dp = self.dropout(embedded_dp)\n",
        "      embedded_cp = self.dropout(embedded_cp)\n",
        "\n",
        "      # Attention\n",
        "      ## output dim: batch_size x (embedding_dim+1)\n",
        "      attended_dp, weights_dp = self.attention_dp(embedded_dp, (dp > 0).float())\n",
        "      attended_cp, weights_cp = self.attention_cp(embedded_cp, (cp > 0).float())\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []\n",
        "\n",
        "\n",
        "if net_variant == 'mce_birnn':\n",
        "  # GRU\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))\n",
        "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))\n",
        "\n",
        "      # Precomputed embedding weights\n",
        "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "      self.emb_weight_dp = torch.Tensor(np.load(data_dir + 'emb_weight_dp_7.npy')).to(self.device)\n",
        "      self.emb_weight_cp = torch.Tensor(np.load(data_dir + 'emb_weight_cp_6.npy')).to(self.device)\n",
        "\n",
        "      # GRU layers\n",
        "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
        "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
        "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
        "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(2*(self.embed_dp_dim+1), 1)\n",
        "      self.fc_cp  = nn.Linear(2*(self.embed_cp_dim+1), 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp_fw = F.embedding(dp, self.emb_weight_dp, padding_idx=0)\n",
        "      embedded_cp_fw = F.embedding(cp, self.emb_weight_cp, padding_idx=0)\n",
        "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
        "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
        "      ## Dropout\n",
        "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
        "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
        "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
        "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
        "\n",
        "      # GRU\n",
        "      ## output dim rnn:        batch_size x seq_len x (embedding_dim+1)\n",
        "      ## output dim rnn_hidden: batch_size x 1 x (embedding_dim+1)\n",
        "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(embedded_dp_fw)\n",
        "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(embedded_cp_fw)\n",
        "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(embedded_dp_bw)\n",
        "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(embedded_cp_bw)\n",
        "      ## output dim rnn_hidden: batch_size x (embedding_dim+1)\n",
        "      rnn_hidden_dp_fw = rnn_hidden_dp_fw.view(-1, self.embed_dp_dim+1)\n",
        "      rnn_hidden_cp_fw = rnn_hidden_cp_fw.view(-1, self.embed_cp_dim+1)\n",
        "      rnn_hidden_dp_bw = rnn_hidden_dp_bw.view(-1, self.embed_dp_dim+1)\n",
        "      rnn_hidden_cp_bw = rnn_hidden_cp_bw.view(-1, self.embed_cp_dim+1)\n",
        "      ## concatenate forward and backward: batch_size x 2*(embedding_dim+1)\n",
        "      rnn_hidden_dp = torch.cat((rnn_hidden_dp_fw, rnn_hidden_dp_bw), dim=-1)\n",
        "      rnn_hidden_cp = torch.cat((rnn_hidden_cp_fw, rnn_hidden_cp_bw), dim=-1)\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(rnn_hidden_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(rnn_hidden_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []\n",
        "\n",
        "\n",
        "elif net_variant == 'mce_birnn_attention':\n",
        "  # GRU\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # Embedding dimensions\n",
        "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))\n",
        "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))\n",
        "\n",
        "      # Precomputed embedding weights\n",
        "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "      self.emb_weight_dp = torch.Tensor(np.load(data_dir + 'emb_weight_dp_7.npy')).to(self.device)\n",
        "      self.emb_weight_cp = torch.Tensor(np.load(data_dir + 'emb_weight_cp_6.npy')).to(self.device)\n",
        "\n",
        "      # GRU layers\n",
        "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
        "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
        "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
        "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
        "\n",
        "      # Attention layers\n",
        "      self.attention_dp = Attention(embedding_dim=2*(self.embed_dp_dim+1)) #+1 for the concatenated time\n",
        "      self.attention_cp = Attention(embedding_dim=2*(self.embed_cp_dim+1))\n",
        "\n",
        "      # Fully connected output\n",
        "      self.fc_dp  = nn.Linear(2*(self.embed_dp_dim+1), 1)\n",
        "      self.fc_cp  = nn.Linear(2*(self.embed_cp_dim+1), 1)\n",
        "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "      # Others\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "      # Embedding\n",
        "      ## output dim: batch_size x seq_len x embedding_dim\n",
        "      embedded_dp_fw = F.embedding(dp, self.emb_weight_dp, padding_idx=0)\n",
        "      embedded_cp_fw = F.embedding(cp, self.emb_weight_cp, padding_idx=0)\n",
        "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
        "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
        "      ## Dropout\n",
        "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
        "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
        "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
        "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
        "\n",
        "      # GRU\n",
        "      ## output dim rnn:        batch_size x seq_len x (embedding_dim+1)\n",
        "      ## output dim rnn_hidden: batch_size x 1 x (embedding_dim+1)\n",
        "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(embedded_dp_fw)\n",
        "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(embedded_cp_fw)\n",
        "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(embedded_dp_bw)\n",
        "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(embedded_cp_bw)\n",
        "      # concatenate forward and backward\n",
        "      ## output dim: batch_size x seq_len x 2*(embedding_dim+1)\n",
        "      rnn_dp = torch.cat((rnn_dp_fw, torch.flip(rnn_dp_bw, [1])), dim=-1)\n",
        "      rnn_cp = torch.cat((rnn_cp_fw, torch.flip(rnn_cp_bw, [1])), dim=-1)\n",
        "\n",
        "      # Attention\n",
        "      ## output dim: batch_size x 2*(embedding_dim+1)\n",
        "      attended_dp, weights_dp = self.attention_dp(rnn_dp, (dp > 0).float())\n",
        "      attended_cp, weights_cp = self.attention_cp(rnn_cp, (cp > 0).float())\n",
        "\n",
        "      # Scores\n",
        "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
        "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
        "\n",
        "      # Concatenate to variable collection\n",
        "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "      # Final linear projection\n",
        "      out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "      return out, []"
      ],
      "metadata": {
        "id": "rszLzzKYa2PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "8sQZ61-udsAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Load data...')\n",
        "data = np.load(data_dir + 'data_arrays.npz')\n",
        "\n",
        "trainloader, num_batches, pos_weight = get_trainloader(data, 'TRAIN')\n",
        "\n",
        "static = num_statics(data)\n",
        "num_dp_codes, num_cp_codes = vocab_sizes(data)\n",
        "\n",
        "print('-----------------------------------------')\n",
        "print('Train...')\n",
        "\n",
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Network\n",
        "net = Net(static, num_dp_codes, num_cp_codes).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
        "\n",
        "# Store times\n",
        "epoch_times = []\n",
        "\n",
        "# Train\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  # print('-----------------------------------------')\n",
        "  # print('Epoch: {}'.format(epoch))\n",
        "  net.train()\n",
        "  time_start = time()\n",
        "  for i, (stat, dp, cp, dp_t, cp_t, label) in enumerate(tqdm(trainloader), 0):\n",
        "    # move to GPU if available\n",
        "    stat  = stat.to(device)\n",
        "    dp    = dp.to(device)\n",
        "    cp    = cp.to(device)\n",
        "    dp_t  = dp_t.to(device)\n",
        "    cp_t  = cp_t.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    label_pred, _ = net(stat, dp, cp, dp_t, cp_t)\n",
        "    loss = criterion(label_pred, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# timing\n",
        "time_end = time()\n",
        "epoch_times.append(time_end-time_start)"
      ],
      "metadata": {
        "id": "SJjIjALGdt8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Saving...')\n",
        "torch.save(net.state_dict(), logdir + net_variant+ '.pt')\n",
        "np.savez(logdir + net_variant, epoch_times=epoch_times)\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "ppkHwh7ZSTzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "OdZJ9YavogQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import scipy.stats as st\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "#import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import *\n",
        "from sklearn.calibration import calibration_curve\n",
        "from pdb import set_trace as bp\n",
        "\n",
        "def round(num):\n",
        "  return np.round(num*1000)/1000\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # Load data\n",
        "  print('Load data...')\n",
        "  data = np.load(data_dir + 'data_arrays.npz')\n",
        "  test_ids_patients = pd.read_pickle(data_dir + 'test_ids_patients.pkl')\n",
        "\n",
        "  # Patients in test data\n",
        "  patients = test_ids_patients.drop_duplicates()\n",
        "  num_patients = patients.shape[0]\n",
        "  row_ids = pd.DataFrame({'ROW_IDX': test_ids_patients.index}, index=test_ids_patients)\n",
        "\n",
        "  # Vocabulary sizes\n",
        "  num_static = num_statics(data)\n",
        "  num_dp_codes, num_cp_codes = vocab_sizes(data)\n",
        "\n",
        "  # CUDA for PyTorch\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "\n",
        "  # Network\n",
        "  net = Net(num_static, num_dp_codes, num_cp_codes).to(device)\n",
        "\n",
        "  print('Evaluate...')\n",
        "  # Set log dir to read trained model from\n",
        "  model = logdir + net_variant +'.pt'\n",
        "\n",
        "  # Restore variables from disk\n",
        "  net.load_state_dict(torch.load(model, map_location=device))\n",
        "\n",
        "  # Bootstrapping\n",
        "  np.random.seed(np_seed)\n",
        "  avpre_vec = np.zeros(bootstrap_samples)\n",
        "  auroc_vec = np.zeros(bootstrap_samples)\n",
        "  f1_vec    = np.zeros(bootstrap_samples)\n",
        "  sensitivity_vec = np.zeros(bootstrap_samples)\n",
        "  specificity_vec = np.zeros(bootstrap_samples)\n",
        "  ppv_vec = np.zeros(bootstrap_samples)\n",
        "  npv_vec = np.zeros(bootstrap_samples)\n",
        "\n",
        "  for sample in range(bootstrap_samples):\n",
        "    print('Bootstrap sample {}'.format(sample))\n",
        "\n",
        "    # Test data\n",
        "    sample_patients = patients.sample(n=num_patients, replace=True)\n",
        "    idx = np.squeeze(row_ids.loc[sample_patients].values)\n",
        "    testloader, _, _ = get_trainloader(data, 'TEST', shuffle=False, idx=idx)\n",
        "\n",
        "    # evaluate on test data\n",
        "    net.eval()\n",
        "    label_pred = torch.Tensor([])\n",
        "    label_test = torch.Tensor([])\n",
        "    with torch.no_grad():\n",
        "      for i, (stat, dp, cp, dp_t, cp_t, label_batch) in enumerate(tqdm(testloader), 0):\n",
        "        # move to GPU if available\n",
        "        stat  = stat.to(device)\n",
        "        dp    = dp.to(device)\n",
        "        cp    = cp.to(device)\n",
        "        dp_t  = dp_t.to(device)\n",
        "        cp_t  = cp_t.to(device)\n",
        "\n",
        "        label_pred_batch, _ = net(stat, dp, cp, dp_t, cp_t)\n",
        "        label_pred = torch.cat((label_pred, label_pred_batch.cpu()))\n",
        "        label_test = torch.cat((label_test, label_batch))\n",
        "\n",
        "    label_sigmoids = torch.sigmoid(label_pred).cpu().numpy()\n",
        "\n",
        "    # Average precision\n",
        "    avpre = average_precision_score(label_test, label_sigmoids)\n",
        "\n",
        "    # Determine AUROC score\n",
        "    auroc = roc_auc_score(label_test, label_sigmoids)\n",
        "\n",
        "    # Sensitivity, specificity\n",
        "    fpr, tpr, thresholds = roc_curve(label_test, label_sigmoids)\n",
        "    youden_idx = np.argmax(tpr - fpr)\n",
        "    sensitivity = tpr[youden_idx]\n",
        "    specificity = 1-fpr[youden_idx]\n",
        "\n",
        "    # F1, PPV, NPV score\n",
        "    f1 = 0\n",
        "    ppv = 0\n",
        "    npv = 0\n",
        "    for t in thresholds:\n",
        "      label_pred = (np.array(label_sigmoids) >= t).astype(int)\n",
        "      f1_temp = f1_score(label_test, label_pred)\n",
        "      ppv_temp = precision_score(label_test, label_pred, pos_label=1)\n",
        "      npv_temp = precision_score(label_test, label_pred, pos_label=0)\n",
        "      if f1_temp > f1:\n",
        "        f1 = f1_temp\n",
        "      if (ppv_temp+npv_temp) > (ppv+npv):\n",
        "        ppv = ppv_temp\n",
        "        npv = npv_temp\n",
        "\n",
        "    # Store in vectors\n",
        "    avpre_vec[sample] = avpre\n",
        "    auroc_vec[sample] = auroc\n",
        "    f1_vec[sample]    = f1\n",
        "    sensitivity_vec[sample]  = sensitivity\n",
        "    specificity_vec[sample]  = specificity\n",
        "    ppv_vec[sample]  = ppv\n",
        "    npv_vec[sample]  = npv\n",
        "\n",
        "  avpre_mean = np.mean(avpre_vec)\n",
        "  avpre_lci, avpre_uci = st.t.interval(0.95, bootstrap_samples-1, loc=avpre_mean, scale=st.sem(avpre_vec))\n",
        "  auroc_mean = np.mean(auroc_vec)\n",
        "  auroc_lci, auroc_uci = st.t.interval(0.95, bootstrap_samples-1, loc=auroc_mean, scale=st.sem(auroc_vec))\n",
        "  f1_mean = np.mean(f1_vec)\n",
        "  f1_lci, f1_uci = st.t.interval(0.95,bootstrap_samples-1, loc=f1_mean, scale=st.sem(f1_vec))\n",
        "  ppv_mean = np.mean(ppv_vec)\n",
        "  ppv_lci, ppv_uci = st.t.interval(0.95, bootstrap_samples-1, loc=ppv_mean, scale=st.sem(ppv_vec))\n",
        "  npv_mean = np.mean(npv_vec)\n",
        "  npv_lci, npv_uci = st.t.interval(0.95, bootstrap_samples-1, loc=npv_mean, scale=st.sem(npv_vec))\n",
        "  sensitivity_mean = np.mean(sensitivity_vec)\n",
        "  sensitivity_lci, sensitivity_uci = st.t.interval(0.95, bootstrap_samples-1, loc=sensitivity_mean, scale=st.sem(sensitivity_vec))\n",
        "  specificity_mean = np.mean(specificity_vec)\n",
        "  specificity_lci, specificity_uci = st.t.interval(0.95, bootstrap_samples-1, loc=specificity_mean, scale=st.sem(specificity_vec))\n",
        "\n",
        "  epoch_times = np.load(logdir + net_variant + '.npz')['epoch_times']\n",
        "  times_mean = np.mean(epoch_times)\n",
        "  times_lci, times_uci = st.t.interval(0.95, len(epoch_times)-1, loc=np.mean(epoch_times), scale=st.sem(epoch_times))\n",
        "  times_std = np.std(epoch_times)\n",
        "\n",
        "  print('------------------------------------------------')\n",
        "  print('Net variant: {}'.format(net_variant))\n",
        "  print('Average Precision: {} [{},{}]'.format(round(avpre_mean), round(avpre_lci), round(avpre_uci)))\n",
        "  print('AUROC: {} [{},{}]'.format(round(auroc_mean), round(auroc_lci), round(auroc_uci)))\n",
        "  print('F1: {} [{},{}]'.format(round(f1_mean), round(f1_lci), round(f1_uci)))\n",
        "  print('PPV: {} [{},{}]'.format(round(ppv_mean), round(ppv_lci), round(ppv_uci)))\n",
        "  print('NPV: {} [{},{}]'.format(round(npv_mean), round(npv_lci), round(npv_uci)))\n",
        "  print('Sensitivity: {} [{},{}]'.format(round(sensitivity_mean), round(sensitivity_lci), round(sensitivity_uci)))\n",
        "  print('Specificity: {} [{},{}]'.format(round(specificity_mean), round(specificity_lci), round(specificity_uci)))\n",
        "  print('Time: {} [{},{}] std: {}'.format(round(times_mean), round(times_lci), round(times_uci), round(times_std)))\n",
        "  print('Done')"
      ],
      "metadata": {
        "id": "wz5eFUqiofBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "def round(num):\n",
        "  return np.round(num*1000)/1000\n",
        "\n",
        "if 1 ==1:\n",
        "  # Load icu_pat table\n",
        "  print('Loading data...')\n",
        "  icu_pat = pd.read_pickle(data_dir + 'icu_pat_admit.pkl')\n",
        "\n",
        "  print('Loading last vital signs measurements...')\n",
        "  charts = pd.read_pickle(data_dir + 'charts_outputs_last_only.pkl')\n",
        "  charts = charts.drop(columns=['CHARTTIME'])\n",
        "  charts = pd.get_dummies(charts, columns = ['VALUECAT']).groupby('ICUSTAY_ID').sum()\n",
        "  charts.drop(columns=['VALUECAT_CHART_BP_n', 'VALUECAT_CHART_BT_n', 'VALUECAT_CHART_GC_n', 'VALUECAT_CHART_HR_n', 'VALUECAT_CHART_RR_n', 'VALUECAT_CHART_UO_n'], inplace=True) # drop reference columns\n",
        "\n",
        "  print('-----------------------------------------')\n",
        "\n",
        "  print('Create array of static variables...')\n",
        "\n",
        "  num_icu_stays = len(icu_pat['ICUSTAY_ID'])\n",
        "\n",
        "  # static variables\n",
        "  print('Create static array...')\n",
        "  icu_pat = pd.get_dummies(icu_pat, columns = ['ADMISSION_LOCATION', 'INSURANCE', 'MARITAL_STATUS', 'ETHNICITY'])\n",
        "  icu_pat.drop(columns=['ADMISSION_LOCATION_Emergency Room Admit', 'INSURANCE_Medicare', 'MARITAL_STATUS_Married/Life Partner', 'ETHNICITY_White'], inplace=True) # drop reference columns\n",
        "\n",
        "  # merge with last vital signs measurements\n",
        "  icu_pat = pd.merge(icu_pat, charts, how='left', on='ICUSTAY_ID').fillna(0)\n",
        "\n",
        "  static_columns = icu_pat.columns.str.contains('AGE|GENDER_M|LOS|NUM_RECENT_ADMISSIONS|ADMISSION_LOCATION|INSURANCE|MARITAL_STATUS|ETHNICITY|PRE_ICU_LOS|ELECTIVE_SURGERY|VALUECAT')\n",
        "  static = icu_pat.loc[:, static_columns].values\n",
        "  static_vars = icu_pat.loc[:, static_columns].columns.values.tolist()\n",
        "\n",
        "  # classification label\n",
        "  print('Create label array...')\n",
        "  label = icu_pat.loc[:, 'POSITIVE'].values\n",
        "\n",
        "  print('-----------------------------------------')\n",
        "\n",
        "  print('Split data into train/validate/test...')\n",
        "  # Split patients to avoid data leaks\n",
        "  patients = icu_pat['SUBJECT_ID'].drop_duplicates()\n",
        "  train, validate, test = np.split(patients.sample(frac=1, random_state=123), [int(.9*len(patients)), int(.9*len(patients))])\n",
        "  train_ids = icu_pat['SUBJECT_ID'].isin(train).values\n",
        "  test_ids = icu_pat['SUBJECT_ID'].isin(test).values\n",
        "\n",
        "  data_train = static[train_ids, :]\n",
        "  data_test = static[test_ids, :]\n",
        "\n",
        "  label_train = label[train_ids]\n",
        "  label_test = label[test_ids]\n",
        "\n",
        "  # Patients in test data\n",
        "  test_ids_patients = pd.read_pickle(data_dir + 'test_ids_patients.pkl')\n",
        "  patients = test_ids_patients.drop_duplicates()\n",
        "  num_patients = patients.shape[0]\n",
        "  row_ids = pd.DataFrame({'ROW_IDX': test_ids_patients.index}, index=test_ids_patients)\n",
        "\n",
        "  print('-----------------------------------------')\n",
        "\n",
        "  # Fit logistic regression model\n",
        "  print('Fit logistic regression model...')\n",
        "  regr = linear_model.LogisticRegression()\n",
        "  regr.fit(data_train, label_train)\n",
        "\n",
        "  # Bootstrapping\n",
        "  np.random.seed(np_seed)\n",
        "  avpre_vec = np.zeros(bootstrap_samples)\n",
        "  auroc_vec = np.zeros(bootstrap_samples)\n",
        "  f1_vec    = np.zeros(bootstrap_samples)\n",
        "  sensitivity_vec = np.zeros(bootstrap_samples)\n",
        "  specificity_vec = np.zeros(bootstrap_samples)\n",
        "  ppv_vec = np.zeros(bootstrap_samples)\n",
        "  npv_vec = np.zeros(bootstrap_samples)\n",
        "\n",
        "  for sample in range(bootstrap_samples):\n",
        "    print('Bootstrap sample {}'.format(sample))\n",
        "\n",
        "    sample_patients = patients.sample(n=num_patients, replace=True)\n",
        "    idx = np.squeeze(row_ids.loc[sample_patients].values)\n",
        "    data_test_bs, label_test_bs = data_test[idx], label_test[idx]\n",
        "\n",
        "    label_sigmoids = regr.predict_proba(data_test_bs)[:, 1]\n",
        "\n",
        "    print('Evaluate...')\n",
        "    # Average precision\n",
        "    avpre = average_precision_score(label_test_bs, label_sigmoids)\n",
        "\n",
        "    # Determine AUROC score\n",
        "    auroc = roc_auc_score(label_test_bs, label_sigmoids)\n",
        "\n",
        "    # Sensitivity, specificity\n",
        "    fpr, tpr, thresholds = roc_curve(label_test_bs, label_sigmoids)\n",
        "    youden_idx = np.argmax(tpr - fpr)\n",
        "    sensitivity = tpr[youden_idx]\n",
        "    specificity = 1-fpr[youden_idx]\n",
        "\n",
        "    # F1, PPV, NPV score\n",
        "    f1 = 0\n",
        "    ppv = 0\n",
        "    npv = 0\n",
        "    for t in thresholds:\n",
        "      label_pred = (np.array(label_sigmoids) >= t).astype(int)\n",
        "      f1_temp = f1_score(label_test_bs, label_pred)\n",
        "      ppv_temp = precision_score(label_test_bs, label_pred, pos_label=1)\n",
        "      npv_temp = precision_score(label_test_bs, label_pred, pos_label=0)\n",
        "      if f1_temp > f1:\n",
        "        f1 = f1_temp\n",
        "      if (ppv_temp+npv_temp) > (ppv+npv):\n",
        "        ppv = ppv_temp\n",
        "        npv = npv_temp\n",
        "\n",
        "    # Store in vectors\n",
        "    avpre_vec[sample] = avpre\n",
        "    auroc_vec[sample] = auroc\n",
        "    f1_vec[sample]    = f1\n",
        "    sensitivity_vec[sample]  = sensitivity\n",
        "    specificity_vec[sample]  = specificity\n",
        "    ppv_vec[sample]  = ppv\n",
        "    npv_vec[sample]  = npv\n",
        "\n",
        "  avpre_mean = np.mean(avpre_vec)\n",
        "  avpre_lci, avpre_uci = st.t.interval(0.95, bootstrap_samples-1, loc=avpre_mean, scale=st.sem(avpre_vec))\n",
        "  auroc_mean = np.mean(auroc_vec)\n",
        "  auroc_lci, auroc_uci = st.t.interval(0.95, bootstrap_samples-1, loc=auroc_mean, scale=st.sem(auroc_vec))\n",
        "  f1_mean = np.mean(f1_vec)\n",
        "  f1_lci, f1_uci = st.t.interval(0.95, bootstrap_samples-1, loc=f1_mean, scale=st.sem(f1_vec))\n",
        "  ppv_mean = np.mean(ppv_vec)\n",
        "  ppv_lci, ppv_uci = st.t.interval(0.95, bootstrap_samples-1, loc=ppv_mean, scale=st.sem(ppv_vec))\n",
        "  npv_mean = np.mean(npv_vec)\n",
        "  npv_lci, npv_uci = st.t.interval(0.95, bootstrap_samples-1, loc=npv_mean, scale=st.sem(npv_vec))\n",
        "  sensitivity_mean = np.mean(sensitivity_vec)\n",
        "  sensitivity_lci, sensitivity_uci = st.t.interval(0.95, bootstrap_samples-1, loc=sensitivity_mean, scale=st.sem(sensitivity_vec))\n",
        "  specificity_mean = np.mean(specificity_vec)\n",
        "  specificity_lci, specificity_uci = st.t.interval(0.95, bootstrap_samples-1, loc=specificity_mean, scale=st.sem(specificity_vec))\n",
        "\n",
        "  print('------------------------------------------------')\n",
        "  print('Net variant: logistic regression')\n",
        "  print('Average Precision: {} [{},{}]'.format(round(avpre_mean), round(avpre_lci), round(avpre_uci)))\n",
        "  print('AUROC: {} [{},{}]'.format(round(auroc_mean), round(auroc_lci), round(auroc_uci)))\n",
        "  print('F1: {} [{},{}]'.format(round(f1_mean), round(f1_lci), round(f1_uci)))\n",
        "  print('PPV: {} [{},{}]'.format(round(ppv_mean), round(ppv_lci), round(ppv_uci)))\n",
        "  print('NPV: {} [{},{}]'.format(round(npv_mean), round(npv_lci), round(npv_uci)))\n",
        "  print('Sensitivity: {} [{},{}]'.format(round(sensitivity_mean), round(sensitivity_lci), round(sensitivity_uci)))\n",
        "  print('Specificity: {} [{},{}]'.format(round(specificity_mean), round(specificity_lci), round(specificity_uci)))\n",
        "  print('Done')"
      ],
      "metadata": {
        "id": "P-ZB0ouekI-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "At the end of this project, I have attempted to replicate the results of the orginal paper and test the 2 main hypotheses.\n",
        "\n",
        "1.  Evaluate the feasibility of using neural ODEs to model how the predictive relevance of recorded medical codes changes over time\n",
        "I will implement and run ODE architecture models to determine the power of ODEs with time data.\n",
        "\n",
        "  To this end, I have implemented neural ODEs to model the nuances in time of medical code embeddings. These neural ODEs should have provided better insight and weight to variables such as length of stay and change in vital signs over time. From the table below, we can conclude that the inclusion of ODE architecture led to a sizable increase in AUROC. In particular, the ODE architecture outperformed the base logistic regression that is being used as a baseline. However, the use of decay gates caused the models to perform worse. This is true for both the ODE models and non-ODE models. In conclusion, neural ODEs have merit in predictive model architecture but should be used in isolation from decay gates.\n",
        "\n",
        "2. Perform a comprehensive comparison of deep learning models that have been proposed for processing time-series sampled at irregular intervals, including MCEs, neural ODEs, attention mechanisms, and recurrent layers I will implement various types of architectures and compare them against each other using metrics such as: precision, AUROC, and F1 score\n",
        "\n",
        "  I have implemented the 14 models outlined in this model including the baseline logistic regression. They have been compared against each other using precision, AUROC, and F1 score. Overall, the results of the various architectures had consistent results with each other.\n",
        "  \n",
        "  Decay functionality that took into account time-related information was applied by instating an exponential decay to the time differences between observations to the internal memory state of the recurrent cell. This approach to time embeddings caused most models that contianed this architecture to perform significantly worse than their counterparts. This can be clearly seen with the results of the birnn_time_decay model.\n",
        "  \n",
        "  ODE architecture as mentioned above seemed to perform best in most models. Modeling the time dynamics in the memory state of the recurrent cells using ODEs proved to be far more effective than decay for the purpose of representing time in RNN models.\n",
        "\n",
        "  MCE or medical concept embeddings provided marginal increases to the accuracy of these models. Pretrained MCEs provided by the original paper assisted the two models utilizing MCEs to outperform all other models aside from the ODE attention models.\n",
        "\n",
        "  Attention mechanisms that allowed these models to focus on specific inputs was also a winning portion of the architecture as all attention models outperformed their non-atttention counterparts. This can be most clearly seen in the increase in precision of the birnn_time_decay models.\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Table 1\n",
        "\n",
        " Summary statistics for the different algorithms used to predict readmission within 30 days of discharge from the intensive care unit.\n",
        "\n",
        "Model | Precision | AUROC | F1\n",
        "-------------------|------------------|-----------------|-------------\n",
        "birnn_concat_time_delta | 0.277 | 0.652 | 0.297\n",
        "birnn_concat_time_delta_attention | 0.262 | 0.618 | 0.282\n",
        "birnn_time_decay |0.136 | 0.516|0.227\n",
        "birnn_time_decay_attention |0.295 | 0.691|0.323\n",
        "ode_birnn |0.293 | 0.693|0.328\n",
        "ode_birnn_attention |0.289 |0.698 |0.336\n",
        "ode_attention |0.305 | 0.707|0.338\n",
        "attention_concat_time |0.258 | 0.625|0.285\n",
        "birnn_ode_decay |0.241 | 0.64|0.284\n",
        "birnn_ode_decay_attention |0.263 | 0.633|0.288\n",
        "mce_attention |0.276 | 0.669|0.306\n",
        "mce_birnn_attention|0.272 | 0.673|0.314\n",
        "logisitic regression | 0.265 | 0.658 | 0.296"
      ],
      "metadata": {
        "id": "5sY89L6QBIo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "Make assessment that the paper is reproducible or\n",
        "not.\n",
        "∗ Explain why it is not reproducible if your results\n",
        "are kind negative.\n",
        "∗ Describe “What was easy” and “What was\n",
        "difficult” during the reproduction.\n",
        "∗ Make suggestions to the author or other\n",
        "reproducers on how to improve the reproducibility.\n",
        "\n",
        "-------------------------------------------------------------------------------\n",
        "\n",
        "The paper is reproducible though it is difficult to do within the training time constraint as this particular paper requires the training of multiple models. In this demo, I used a small set of data in order to get a reasonable training time. In addition, I elected to upload preprocessed data from the original paper's codebase as the processing of the data is 1. too time consuming and 2. not the main focus of the paper. I believe that further training with hyperparameter tuning could lead to better performances than what is displayed here and in the paper. However, for some of these architectures, the complexity renders the amount of calculations to exceed what is reasonable to be replicated on base versions of colab.\n",
        "\n",
        "The easy portions of the reproduction of this paper were the MCEs as they were provided for us as pretrained embeddings. In addition, the usage of attention mechanisms were not foreign and mostly straightfoward in its implementation.\n",
        "\n",
        "The difficult portions of this paper were the neural ODEs as well as the decay functionality. The orignal dataset had to be slightly modified to create data that the de novo ODE and decay cell architecture could accept. This led to awkward data restructuring that caused the largest portion of delays in this project. The ODE decay combination cell was particularly difficult to understand. The decay of the hidden state did not feel intuitive as the decay was computed by an ODE.\n",
        "\n",
        "To the authors, I would recommend limiting the scope of the architectures for the sake of reproducibility. In terms of difficulty, the understanding of the time decay models was by far the most difficult. For this notebook, I recommend that the graders run the model that I have left uncommented for validation."
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1. Barbieri, S., Kemp, J., Perez-Concha, O. et al. Benchmarking Deep Learning Architectures for Predicting Readmission to the ICU and Describing Patients-at-Risk. Sci Rep 10, 1111 (2020). https://doi.org/10.1038/s41598-020-58053-z\n",
        "2. Cai, X. et al. Medical concept embedding with time-aware attention. arXiv preprint arXiv:1806.02873 (2018).\n",
        "3. Rubanova, Y., Chen, R. T. & Duvenaud, D. Latent odes for irregularly-sampled time series. arXiv preprint arXiv:1907.03907 (2019).\n",
        "4. Mozer, M. C., Kazakov, D. & Lindsey, R. V. Discrete event, continuous time rnns. arXiv preprint arXiv:1710.04110 (2017).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}